= Appendix: Lab Details

== Two ways to `oc-mirror` - Partially Disconnected -vs Fully Disconnected
The `oc-mirror` plugin supports two mirroring scenarios.

If your network security allows it, `oc-mirror` can simultaneously download the OpenShift installation images and upload it into the [.highside]#`mirror-registry` in the disconnected environment#.

This is documented as a https://docs.openshift.com/container-platform/4.15/installing/disconnected_install/installing-mirroring-disconnected.html#mirroring-image-set-partial[partially disconnected environment,window=_blank].

The other scenario, described in this workshop, requires `oc-mirror` to download the OpenShift installation images into a .tar file on a [.lowside]#lowside system#.

After the .tar file is created, it must be moved into the [.highside]# highside disconnected environment#.

Another instance of `oc-mirror` will open the .tar file and upload the contents into the `mirror-registry`.

This is documented as a https://docs.openshift.com/container-platform/4.15/installing/disconnected_install/installing-mirroring-disconnected.html#mirroring-image-set-full[fully disconnected environment,window=_blank].


== Creating an Air Gap

According to the https://www.rfc-editor.org/rfc/rfc4949[Internet Security Glossary], an Air Gap is an interface between two systems at which (a) they are not connected physically and (b) any logical connection is not automated (i.e., data is transferred through the interface only manually, under human control).
In disconnected installations, the air gap exists between the Low Side and the High Side, so it is between these systems where a manual data transfer, or *sneaker net* is required.

== Preparing the Low Side

A disconnected installation begins with downloading content and tooling to a system we will refer to as the *jump* that has outbound access to the Internet.
This server resides in a network environment commonly referred to as the *Low Side* due to its low security profile.
Compliance requirements usually prevent the low side from housing sensitive data or private information.

== Preparing the High Side

We then provision a system we refer to as *highside* which has no Internet access and resides in a network environment commonly referred to as the *High Side* due to its high security profile.
It is in the high side where sensitive data and production systems (like the cluster we're about to provision) live.
During this phase, we'll transfer the content and tooling from the low side to the high side, a process which may entail use of a VPN or physical media like a DVD or USB.
Once our transfer is complete, we'll start a *mirror registry* on the highside system which will supply the container images needed for our OpenShift installation, since our installer can't reach out to quay.io or registry.redhat.io directly from here.

=== Proxy

The high side protects outbound traffic with a http://www.squid-cache.org/[Squid proxy] running on the `nat system`.
The `nat system / proxy` prevents any egress traffic not listed in its `/etc/squid/whitelist.txt` file.

Note that access from the [.highside]#highside systems# to quay.io, openshift.com, and redhat.com is **DENIED**.

[.lowside,source,bash,role=execute]
----
ssh nat "sudo cat /etc/squid/whitelist.txt"
----
[.output]
----
.amazonaws.com
.cloudfront.net
.aws.ce.redhat.com
----

There may be situations where you wish to add more exceptions here, such as container or package repositories.

Now that our air gap is in place, let's start prepping the low side.


== Preparing the Installation

The OpenShift installation process is initiated from the *highside* server.
There are a handful of different ways to install OpenShift, but for this lab we're going to be using installer-provisioned infrastructure (IPI).
By default, the installation program acts as an installation wizard, prompting you for values that it cannot determine on its own and providing reasonable default values for the remaining parameters.
We'll then customize the `install-config.yaml` file that is produced to specify advanced configuration for our disconnected installation.
The installation program then provisions the underlying infrastructure for the cluster.

[TIP]
You may notice that nodes are provisioned through a process called _Ignition_.
This concept is out of scope for this workshop, but if you're interested to learn more about it, you can read up on it in the documentation https://docs.openshift.com/container-platform/4.13/installing/index.html#about-rhcos[here].

IPI is the recommended installation method in most cases because it leverages full automation in installation and cluster management, but there are some key considerations to keep in mind when planning a production installation in a real world scenario.

* *You may not have access to the infrastructure APIs.* Our lab is going to live in AWS, which requires connectivity to the `.amazonaws.com` domain to talk to AWS, as well as the `.aws.ce.redhat.com` domain to talk to Red Hat Update Infrastructure (RHUI).
We accomplish this by using an _allowed list_ on a Squid proxy running on the high side, but a similar approach may not be achievable or permissible for everyone.
We'll discuss this further later in the lab.
* *You may not have sufficient permissions with your infrastructure provider*.
Our lab has full admin in our AWS enclave, so that's not a constraint we'll need to deal with.
In real world environments, you'll need to ensure your account has the https://docs.openshift.com/container-platform/4.13/installing/installing_aws/installing-aws-account.html#installation-aws-permissions_installing-aws-account[appropriate permissions] which sometimes involves negotiating with security teams.

Once configuration has been completed, we can kick off the OpenShift Installer and it will do all the work for us to provision the infrastructure and install OpenShift.
Here's a diagram describing everything we've discussed so far: 

image::disco-1.svg[disco diagram,800]

== Accessing the Cluster

Since the cluster we'll produce is in a disconnected environment, it won't be publicly accessible via the Internet.
In many cases, cluster access is restricted to hosts that reside in the high side network.
In our lab, we'll use the *jump* system to access the cluster running in the high side network.

== Planning Ahead

Once the cluster is up, what comes next?
This lab ends when the cluster is installed, but there are many more considerations to made for how you manage things like upgrades, operators, patches, and more.
