= Post-Install Tasks

**Congratulations** you've successfully created an OpenShift cluster in a disconnected environment!

ðŸª© ðŸ’ƒ It's DISCO time! ðŸ•º ðŸª©

Or, perhaps, your cluster will finish installing in the next ~30 minutes. ðŸ˜¸

Don't worry!
Your workshop environment also includes a previously-built disconnected **Salsa cluster**.
You can complete this lab using either cluster.

image::disco-4.svg[disco diagram,800]

== Are we done yet?

Installing OpenShift in a disconnected environment concludes the __"day 1"__ tasks.
However, a disconnected cluster is more useful when you make a few simple configuration changes.
These are commonly referred to as __"day 2"__ tasks.

Here are a few suggested __"day 2"__ tasks that will improve your experience.

{counter:overview}. Disable the default app / Operator `CatalogSources`

{counter:overview}. Create your own app / Operator `CatalogSource`

{counter:overview}. Install additional `ImageContentSourcePolicies`

{counter:overview}. Update OpenShift from a disconnected _Update Server_

[NOTE]
You may not have enough time during the Summit 2024 Lab Session to complete the __Update Server__ task.

== Accessing your cluster

Because of time constraints, we recommend that you decide now to either use your [.highside]#disco.lab cluster#, or the prebuilt [.salsa]#salsa.lab cluster#.

The instructions are the same - only the names change.

=== Retrieving the default `kubeadmin` password

The OpenShift install process creates a randomized password for the default cluster administrative user, `kubeadmin` and it is printed when `openshift-install` finishes.
However, if you don't want to wait or your terminal output is gone you can lookup the password:

With the [.highside]#highside system#:

[.highside,source,bash,role=execute,subs="attributes"]
----
echo "URL: https://console-openshift-console.apps.disco.lab"
echo "Username: kubeadmin"
echo "Password: $(more /mnt/high-side-data/install/auth/kubeadmin-password)"
----
[.output]
----
URL: https://console-openshift-console.apps.disco.lab
Username: kubeadmin
Password: *****-*****-*****-*****
----

[TIP]
--
The pre-built **salsa.lab** cluster has preconfigured authentication for the purposes of this lab:

* There is a cluster-admin user available as `admin` / `admin`.
* There is also a non-admin user available as `user` / `user`.
--

=== Accessing the cluster with the command-line interface

We can access either cluster from the [.lowside]#jump system#:

[cols="a,a",options="header"]
|===
| Using the pre-built [.salsa]#salsa.lab cluster#
| Using your own [.highside]#disco.lab cluster#

|
[.lowside,source,bash,role=execute]
----
ssh salsa
----
[.salsa,source,bash,role=execute]
----
oc login https://api.salsa.lab:6443 --username admin --password admin
----
|
[.lowside,source,bash,role=execute]
----
ssh highside
----
[.highside,source,bash,role=execute]
----
oc login https://api.disco.lab:6443 --username kubeadmin 
----

|
* *Username:* admin
* *Password:* admin
|
* *Username:* kubeadmin
* *Password:* (that we found above on the [.highside]#highside system#)
|===

[WARNING]
--
The copy-paste commands below assume that you will be using the pre-built [.salsa]#salsa.lab cluster#.
--

=== Accessing the cluster's web console

Now you can click the *Desktop* button on the right-hand side of the lab web interface and use Firefox to login to the OpenShift __Web Console__.
The __Web Console__ is the very last part of OpenShift to come online.
If you are trying to connect to your [.highside]#disco.lab# cluster, and see an "**Application is not available**" message, it means the installation hasn't completed yet.

Your workshop environment includes a Firefox bookmark to quickly connect with the __Web Console__.

image::vnc-disco-openshift-bookmark.png[Screenshot of Desktop with DISCO - OpenShift bookmark highlighted]

The web-based VNC client is a bit klunky when it comes to **copy** and **paste**.
You can copy and paste into the VNC desktop using __noVNC__'s tool ribbon (hidden by default.) 

image::vnc-copy-paste.png[Screenshot of Desktop with noVNC clipboard highlighted]

// === Accessing the cluster from the command-line

// You should be able to access the API server from the *highside* system by leveraging the `kubeconfig` file the installer creates for you:

// [.highside,source,bash,role=execute,subs="attributes"]
// ----
// mkdir -vp $HOME/.kube
// cp -v /mnt/high-side/install/auth/kubeconfig $HOME/.kube/config
// oc status
// ----
// [.output]
// ----
// In project default on server https://api.disco.lab:6443

// svc/openshift - kubernetes.default.svc.cluster.local
// svc/kubernetes - 172.30.0.1:443 -> 6443

// View details with 'oc describe <resource>/<name>' or list resources with 'oc get all'.
// ----

// Now that you're logged in via the command-line or the _Web Console_ we can proceed with the __"day 2"__ tasks.

== Install Operators

=== Add custom catalog sources
// https://docs.openshift.com/container-platform/4.14/installing/installing_aws/installing-restricted-networks-aws-installer-provisioned.html#olm-restricted-networks-operatorhub_installing-restricted-networks-aws-installer-provisioned

In order to deploy Operators from the customized catalog that was created in the `mirror-registry`, we have to add a new `CatalogSource` to OpenShift.
We also need to disable the default CatalogSources.
OpenShift clusters, by default, will try to use the online/connected versions of the Operator Catalogs from registry.redhat.io.

First, we will disable the default sources from the disconnected [.salsa]#salsa.lab cluster#.

[.salsa,source,bash,role=execute]
----
oc patch OperatorHub cluster --type merge -p '{"spec": {"disableAllDefaultSources": true}}'
----

Next, we will add the customized `CatalogSource` that `oc-mirror` created for us.

[INFO]
--
New `CatalogSources` can take 1-5 minutes to appear in the Web Console.
--

[.salsa,source,bash,role=execute]
----
oc create -f oc-mirror-workspace/results-*/catalogSource-cs-redhat-operator-index.yaml
----

Now we should see the Operators that we mirrored over with `oc-mirror` in the OperatorHub on the OpenShift Web Console for [.salsa]#salsa.lab cluster# 

image::disconnected-operator-catalog.png[Screenshot of the disco.lab instance of OpenShift Web Console showing OperatorHub with a custom CatalogSource]

=== Install Web Terminal

Go ahead and install the `Web Terminal` Operator.
Accept all of the default values.
Your disconnected cluster will pull all of the images from your disconnected `mirror-registry`.

After the Web Terminal Operator has been installed, refresh your browser page and see the new Terminal icon at the top of the page.
Clicking the Terminal icon will reveal a Terminal that is logged in with your current credentials and permissions.
The Terminal includes all of the standard `oc`, `kubectl`, `helm`, and `bash`tools.

image::operator-web-terminal.png[Screenshot of the Web Terminal Operator installed and running]

// == Add images with `podman`
// https://docs.openshift.com/container-platform/4.14/post_installation_configuration/cluster-tasks.html#post-install-must-gather-disconnected

// == Remove default samples operator
// https://docs.openshift.com/container-platform/4.14/openshift_images/configuring-samples-operator.html#configuring-samples-operator

== Update your cluster

// First log into the [.salsa]#salsa system#

// [.lowside,source,bash,role=execute]
// ----
// ssh salsa
// ----

// Next log into the [.salsa]#salsa.lab cluster#

// [.salsa,source,bash,role=execute]
// ----
// oc login https://api.salsa.lab:6443 --username admin --password admin
// ----

There are multiple ways to apply updates to your disconnected OpenShift cluster.
The easiest and fastest way to apply updates is to use the `oc adm upgrade` command and reference the new version / release image.

[TIP]
--
OpenShift versions are also known as `releases`.
The list of updates, changes and dependencies each release / version provides is bundled into a `release image` and stored in your `mirror-registry`.
"Applying an OpenShift update" could also be described as "Moving to a new release."
--

Before OpenShift applies any updates, it first checks the update / release's signature to make sure it came from a trusted source.
`oc-mirror` automatically downloadeds signatures for each OpenShift version / release that it downloads.

You upload the release signatures into OpenShift with this command:

[.salsa,source,bash,role=execute]
----
oc apply -f oc-mirror-workspace/results-*/release-signatures
----
[.output]
----
configmap/sha256-08b8725ce619ff1855cb0ec5f5c5baa879ef3c6ab9930db300761b97d2761144 created
configmap/sha256-e64464879cd1acdfa7112c1ac1d90039e1689189e0af197f34881c79decda933 created
----

OpenShift will complain if you tell it to update to a new release using the `tag` name.
OpenShift prefers to update to new releases using the sha256 `digest` because the `digest` guarantees that the `release image` contents haven't been altered.
Please use the following commands to log into the [.salsa]#salsa system#, identify the sha256 `digest` and tell OpenShift to begin the update.

//   Applying a release (by tag) without checking signatures    //
// [lab-user@highside ~]$ oc adm upgrade --to-image=ip-10-0-15-131.us-west-2.compute.internal:8443/openshift/release-images:4.14.20-x86_64 --force --allow-explicit-upgrade
// warning: Using by-tag pull specs is dangerous, and while we still allow it in combination with --force for backward compatibility, it would be much safer to pass a by-digest pull spec instead
// warning: The requested upgrade image is not one of the available updates. You have used --allow-explicit-upgrade for the update to proceed anyway
// warning: --force overrides cluster verification of your supplied release image and waives any update precondition failures.
// Requested update to release image ip-10-0-15-131.us-west-2.compute.internal:8443/openshift/release-images:4.14.20-x86_64

// TODO add callout for other upgrade methods
// TODO talk about getting release image digest from mirror registry directly

[.salsa,source,bash,role=execute]
----
podman login -u init -p salsapass $(hostname):8443
DIGEST=$(oc image info -o json $HOSTNAME:8443/openshift/release-images:4.14.20-x86_64 | jq -r .contentDigest)
oc adm upgrade --to-image=quay.io/openshift-release-dev/ocp-release@$DIGEST --allow-explicit-upgrade
----
[.output]
----
...

Requested update to release image quay.io/openshift-release-dev/ocp-release@sha256:e64464879cd1acdfa7112c1ac1d90039e1689189e0af197f34881c79decda933
----

You can follow along with the OpenShift update by using the following commands:

[.salsa,source,bash,role=execute]
----
oc get clusterversion
----
[.output]
----
NAME      VERSION   AVAILABLE   PROGRESSING   SINCE   STATUS
version   4.14.18   True        True          50s     Working towards 4.14.19: 116 of 860 done (13% complete), waiting on etcd, kube-apiserver
----

[NOTE]
--
Applying cluster updates to the [.salsa]#salsa.lab# and [.highside]#disco.lab# clusters, which are Single Node OpenShift clusters, will result in the cluster becoming periodically unreachable.
Your cluster(s) become unreachable when the update switchs the `console` and `apiserver` pods from the old version to the new version.
OpenShift updates also apply updates to the underlying Operating System, https://docs.openshift.com/container-platform/4.15/architecture/architecture-rhcos.html[Red Hat Enterprise Linux CoreOS,window=_blank].
Your cluster(s) will become unreachable again when the Single Node of OpenShift reboots to apply `kernel` and other critical system updates.
--